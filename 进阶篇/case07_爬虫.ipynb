{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬虫 http请求\n",
    "import requests\n",
    "import json\n",
    "#建立agent池\n",
    "headers={\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36\"\n",
    "}\n",
    "# #fake-user-agent agent池\n",
    "# import fake_useragent\n",
    "# ua=fake_useragent.UserAgent()\n",
    "# headers[\"User-Agent\"]=ua.random\n",
    "#发送请求\n",
    "response=requests.get(\"\",headers=headers,params={\"wd\":\"python\"})\n",
    "response1=requests.post(\"\",headers=headers,data={\"name\":\"张三\"})\n",
    "# #实例化session\n",
    "# session=requests.session()\n",
    "# #发送请求\n",
    "# response=session.get(\"\",headers=headers,params={\"wd\":\"python\"})\n",
    "#获取响应数据\n",
    "print(response.text,enconding=\"utf-8\")\n",
    "print(response.content.decode())\n",
    "with open(\"test.html\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(response.content.decode())\n",
    "#保存响应数据 二进制数据流\n",
    "with open(\"test.html\",\"wb\",encoding=\"utf-8\") as f:\n",
    "    f.write(response.content.decode())\n",
    "#获取响应头\n",
    "print(response.headers)\n",
    "#获取响应状态码\n",
    "print(response.status_code)\n",
    "#获取响应cookies\n",
    "print(response.cookies)\n",
    "\n",
    "#字符串处理 quote\n",
    "import urllib.parse\n",
    "#quote :url编码\n",
    "#unquote :url解码\n",
    "print(urllib.parse.quote(\"你好\"))#%E4%BD%A0%E5%A5%BD\n",
    "print(urllib.parse.unquote(\"%E4%BD%A0%E5%A5%BD\"))#你好\n",
    "\n",
    "#proxy 代理\n",
    "#代理池\n",
    "proxies={\n",
    "    \"http\":\"http://127.0.0.1:8080\",\n",
    "    \"https\":\"https://127.0.0.1:8080\"\n",
    "}\n",
    "#发送请求\n",
    "response=requests.get(\"\",headers=headers,proxies=proxies)\n",
    "\n",
    "# #beautifulsoup 解析html\n",
    "# from bs4 import BeautifulSoup\n",
    "# #实例化BeautifulSoup\n",
    "# soup=BeautifulSoup(response.text,\"html.parser\")\n",
    "# #获取标签列表\n",
    "# print(soup.find_all(\"a\"))#获取所有a标签\n",
    "\n",
    "# #xml 解析 etree\n",
    "# from lxml import etree\n",
    "# #实例化etree\n",
    "# tree=etree.HTML(response.text)\n",
    "# #获取标签列表\n",
    "# print(tree.xpath(\"//a\"))#获取所有a标签\n",
    "# #获取标签属性值\n",
    "# print(tree.xpath(\"//a/@href\"))#获取所有a标签的href属性值"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
