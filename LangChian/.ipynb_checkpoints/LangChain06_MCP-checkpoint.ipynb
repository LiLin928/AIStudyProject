{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b51924-2932-4fe4-a04b-c54871ddc760",
   "metadata": {},
   "source": [
    "MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "856866ad-d9a9-4c23-a6e6-6c0fca0319f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m178 packages\u001b[0m \u001b[2min 2.27s\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 443ms\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 161ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmcp\u001b[0m\u001b[2m==1.9.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msse-starlette\u001b[0m\u001b[2m==2.3.6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# å®‰è£… MCP SDK\n",
    "! uv add mcp openai python-dotenv httpx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c56ff-7fd9-472f-b0f0-5ddb2c73f29b",
   "metadata": {},
   "source": [
    "ç”¨äºå¤©æ°”æŸ¥è¯¢çš„serveræœåŠ¡å™¨ä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d8cd08-34e3-4988-8206-1f25e7c4361e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Already running asyncio in this thread",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m format_weather(data)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# ä»¥æ ‡å‡† I/O æ–¹å¼è¿è¡Œ MCP æœåŠ¡å™¨\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[43mmcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstdio\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\LangChain\\Jupyter\\.venv\\Lib\\site-packages\\mcp\\server\\fastmcp\\server.py:217\u001b[39m, in \u001b[36mFastMCP.run\u001b[39m\u001b[34m(self, transport, mount_path)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mmatch\u001b[39;00m transport:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstdio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[43manyio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_stdio_async\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msse\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    219\u001b[39m         anyio.run(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m.run_sse_async(mount_path))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\LangChain\\Jupyter\\.venv\\Lib\\site-packages\\anyio\\_core\\_eventloop.py:59\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(func, backend, backend_options, *args)\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAlready running \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masynclib_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in this thread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     async_backend = get_async_backend(backend)\n",
      "\u001b[31mRuntimeError\u001b[39m: Already running asyncio in this thread"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import httpx\n",
    "from typing import Any\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "# åˆå§‹åŒ– MCP æœåŠ¡å™¨\n",
    "mcp = FastMCP(\"WeatherServer\")\n",
    "\n",
    "# OpenWeather API é…ç½®\n",
    "OPENWEATHER_API_BASE = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "API_KEY = \"1564bd59e86f981289a646fb2c421a63\"  # è¯·æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ OpenWeather API Key\n",
    "USER_AGENT = \"weather-app/1.0\"\n",
    "\n",
    "async def fetch_weather(city: str) -> dict[str, Any] | None:\n",
    "    \"\"\"\n",
    "    ä» OpenWeather API è·å–å¤©æ°”ä¿¡æ¯ã€‚\n",
    "    :param city: åŸå¸‚åç§°ï¼ˆéœ€ä½¿ç”¨è‹±æ–‡ï¼Œå¦‚ Beijingï¼‰\n",
    "    :return: å¤©æ°”æ•°æ®å­—å…¸ï¼›è‹¥å‡ºé”™è¿”å›åŒ…å« error ä¿¡æ¯çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"q\": city,\n",
    "        \"appid\": API_KEY,\n",
    "        \"units\": \"metric\",\n",
    "        \"lang\": \"zh_cn\"\n",
    "    }\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        try:\n",
    "            response = await client.get(OPENWEATHER_API_BASE, params=params, headers=headers, timeout=30.0)\n",
    "            response.raise_for_status()\n",
    "            return response.json()  # è¿”å›å­—å…¸ç±»å‹\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            return {\"error\": f\"HTTP é”™è¯¯: {e.response.status_code}\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"è¯·æ±‚å¤±è´¥: {str(e)}\"}\n",
    "\n",
    "def format_weather(data: dict[str, Any] | str) -> str:\n",
    "    \"\"\"\n",
    "    å°†å¤©æ°”æ•°æ®æ ¼å¼åŒ–ä¸ºæ˜“è¯»æ–‡æœ¬ã€‚\n",
    "    :param data: å¤©æ°”æ•°æ®ï¼ˆå¯ä»¥æ˜¯å­—å…¸æˆ– JSON å­—ç¬¦ä¸²ï¼‰\n",
    "    :return: æ ¼å¼åŒ–åçš„å¤©æ°”ä¿¡æ¯å­—ç¬¦ä¸²\n",
    "    \"\"\"\n",
    "    # å¦‚æœä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™å…ˆè½¬æ¢ä¸ºå­—å…¸\n",
    "    if isinstance(data, str):\n",
    "        try:\n",
    "            data = json.loads(data)\n",
    "        except Exception as e:\n",
    "            return f\"æ— æ³•è§£æå¤©æ°”æ•°æ®: {e}\"\n",
    "\n",
    "    # å¦‚æœæ•°æ®ä¸­åŒ…å«é”™è¯¯ä¿¡æ¯ï¼Œç›´æ¥è¿”å›é”™è¯¯æç¤º\n",
    "    if \"error\" in data:\n",
    "        return f\"âš ï¸ {data['error']}\"\n",
    "\n",
    "    # æå–æ•°æ®æ—¶åšå®¹é”™å¤„ç†\n",
    "    city = data.get(\"name\", \"æœªçŸ¥\")\n",
    "    country = data.get(\"sys\", {}).get(\"country\", \"æœªçŸ¥\")\n",
    "    temp = data.get(\"main\", {}).get(\"temp\", \"N/A\")\n",
    "    humidity = data.get(\"main\", {}).get(\"humidity\", \"N/A\")\n",
    "    wind_speed = data.get(\"wind\", {}).get(\"speed\", \"N/A\")\n",
    "    # weather å¯èƒ½ä¸ºç©ºåˆ—è¡¨ï¼Œå› æ­¤ç”¨ [0] å‰å…ˆæä¾›é»˜è®¤å­—å…¸\n",
    "    weather_list = data.get(\"weather\", [{}])\n",
    "    description = weather_list[0].get(\"description\", \"æœªçŸ¥\")\n",
    "\n",
    "    return (\n",
    "        f\"ğŸŒ {city}, {country}\\n\"\n",
    "        f\"ğŸŒ¡ æ¸©åº¦: {temp}Â°C\\n\"\n",
    "        f\"ğŸ’§ æ¹¿åº¦: {humidity}%\\n\"\n",
    "        f\"ğŸŒ¬ é£é€Ÿ: {wind_speed} m/s\\n\"\n",
    "        f\"ğŸŒ¤ å¤©æ°”: {description}\\n\"\n",
    "    )\n",
    "\n",
    "@mcp.tool()\n",
    "async def query_weather(city: str) -> str:\n",
    "    \"\"\"\n",
    "    è¾“å…¥æŒ‡å®šåŸå¸‚çš„è‹±æ–‡åç§°ï¼Œè¿”å›ä»Šæ—¥å¤©æ°”æŸ¥è¯¢ç»“æœã€‚\n",
    "    :param city: åŸå¸‚åç§°ï¼ˆéœ€ä½¿ç”¨è‹±æ–‡ï¼‰\n",
    "    :return: æ ¼å¼åŒ–åçš„å¤©æ°”ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    data = await fetch_weather(city)\n",
    "    return format_weather(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ä»¥æ ‡å‡† I/O æ–¹å¼è¿è¡Œ MCP æœåŠ¡å™¨\n",
    "    mcp.run(transport='stdio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c685c9-b0fd-4eaa-84cc-47728ef4f377",
   "metadata": {},
   "source": [
    "- åˆ›å»ºwrite_server.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eaa7174-2fec-4a2e-96a7-63124168d12f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Already running asyncio in this thread",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33må·²æˆåŠŸå†™å…¥æœ¬åœ°æ–‡ä»¶ã€‚\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# ä»¥æ ‡å‡† I/O æ–¹å¼è¿è¡Œ MCP æœåŠ¡å™¨\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43mmcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstdio\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\LangChain\\Jupyter\\.venv\\Lib\\site-packages\\mcp\\server\\fastmcp\\server.py:217\u001b[39m, in \u001b[36mFastMCP.run\u001b[39m\u001b[34m(self, transport, mount_path)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mmatch\u001b[39;00m transport:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstdio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[43manyio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_stdio_async\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msse\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    219\u001b[39m         anyio.run(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m.run_sse_async(mount_path))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\LangChain\\Jupyter\\.venv\\Lib\\site-packages\\anyio\\_core\\_eventloop.py:59\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(func, backend, backend_options, *args)\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAlready running \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masynclib_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in this thread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     async_backend = get_async_backend(backend)\n",
      "\u001b[31mRuntimeError\u001b[39m: Already running asyncio in this thread"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import httpx\n",
    "from typing import Any\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "# åˆå§‹åŒ– MCP æœåŠ¡å™¨\n",
    "mcp = FastMCP(\"WriteServer\")\n",
    "USER_AGENT = \"write-app/1.0\"\n",
    "\n",
    "@mcp.tool()\n",
    "async def write_file(content: str) -> str:\n",
    "    \"\"\"\n",
    "    å°†æŒ‡å®šå†…å®¹å†™å…¥æœ¬åœ°æ–‡ä»¶ã€‚\n",
    "    :param content: å¿…è¦å‚æ•°ï¼Œå­—ç¬¦ä¸²ç±»å‹ï¼Œç”¨äºè¡¨ç¤ºéœ€è¦å†™å…¥æ–‡æ¡£çš„å…·ä½“å†…å®¹ã€‚\n",
    "    :returnï¼šæ˜¯å¦æˆåŠŸå†™å…¥\n",
    "    \"\"\"\n",
    "    return \"å·²æˆåŠŸå†™å…¥æœ¬åœ°æ–‡ä»¶ã€‚\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ä»¥æ ‡å‡† I/O æ–¹å¼è¿è¡Œ MCP æœåŠ¡å™¨\n",
    "    mcp.run(transport='stdio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95281c1-e021-490c-817b-f96844f1533d",
   "metadata": {},
   "source": [
    "- å¤©æ°”æŸ¥è¯¢å®¢æˆ·ç«¯clientåˆ›å»ºæµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b7d27-7aa6-481c-ba23-e70aa6ccb49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "from contextlib import AsyncExitStack\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import httpx\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI  # OpenAI Python SDK\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# é…ç½®åŠ è½½ç±»ï¼ˆæ”¯æŒç¯å¢ƒå˜é‡åŠé…ç½®æ–‡ä»¶ï¼‰\n",
    "# =============================\n",
    "class Configuration:\n",
    "    \"\"\"ç®¡ç† MCP å®¢æˆ·ç«¯çš„ç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        load_dotenv()\n",
    "        # ä»ç¯å¢ƒå˜é‡ä¸­åŠ è½½ API key, base_url å’Œ model\n",
    "        self.api_key = os.getenv(\"LLM_API_KEY\")\n",
    "        self.base_url = os.getenv(\"BASE_URL\")\n",
    "        self.model = os.getenv(\"MODEL\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"âŒ æœªæ‰¾åˆ° LLM_API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_config(file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        ä» JSON æ–‡ä»¶åŠ è½½æœåŠ¡å™¨é…ç½®\n",
    "        \n",
    "        Args:\n",
    "            file_path: JSON é…ç½®æ–‡ä»¶è·¯å¾„\n",
    "        \n",
    "        Returns:\n",
    "            åŒ…å«æœåŠ¡å™¨é…ç½®çš„å­—å…¸\n",
    "        \"\"\"\n",
    "        with open(file_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# MCP æœåŠ¡å™¨å®¢æˆ·ç«¯ç±»\n",
    "# =============================\n",
    "class Server:\n",
    "    \"\"\"ç®¡ç†å•ä¸ª MCP æœåŠ¡å™¨è¿æ¥å’Œå·¥å…·è°ƒç”¨\"\"\"\n",
    "\n",
    "    def __init__(self, name: str, config: Dict[str, Any]) -> None:\n",
    "        self.name: str = name\n",
    "        self.config: Dict[str, Any] = config\n",
    "        self.session: Optional[ClientSession] = None\n",
    "        self.exit_stack: AsyncExitStack = AsyncExitStack()\n",
    "        self._cleanup_lock = asyncio.Lock()\n",
    "\n",
    "    async def initialize(self) -> None:\n",
    "        \"\"\"åˆå§‹åŒ–ä¸ MCP æœåŠ¡å™¨çš„è¿æ¥\"\"\"\n",
    "        # command å­—æ®µç›´æ¥ä»é…ç½®è·å–\n",
    "        command = self.config[\"command\"]\n",
    "        if command is None:\n",
    "            raise ValueError(\"command ä¸èƒ½ä¸ºç©º\")\n",
    "\n",
    "        server_params = StdioServerParameters(\n",
    "            command=command,\n",
    "            args=self.config[\"args\"],\n",
    "            env={**os.environ, **self.config[\"env\"]} if self.config.get(\"env\") else None,\n",
    "        )\n",
    "        try:\n",
    "            stdio_transport = await self.exit_stack.enter_async_context(\n",
    "                stdio_client(server_params)\n",
    "            )\n",
    "            read_stream, write_stream = stdio_transport\n",
    "            session = await self.exit_stack.enter_async_context(\n",
    "                ClientSession(read_stream, write_stream)\n",
    "            )\n",
    "            await session.initialize()\n",
    "            self.session = session\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error initializing server {self.name}: {e}\")\n",
    "            await self.cleanup()\n",
    "            raise\n",
    "\n",
    "    async def list_tools(self) -> List[Any]:\n",
    "        \"\"\"è·å–æœåŠ¡å™¨å¯ç”¨çš„å·¥å…·åˆ—è¡¨\n",
    "\n",
    "        Returns:\n",
    "            å·¥å…·åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        if not self.session:\n",
    "            raise RuntimeError(f\"Server {self.name} not initialized\")\n",
    "        tools_response = await self.session.list_tools()\n",
    "        tools = []\n",
    "        for item in tools_response:\n",
    "            if isinstance(item, tuple) and item[0] == \"tools\":\n",
    "                for tool in item[1]:\n",
    "                    tools.append(Tool(tool.name, tool.description, tool.inputSchema))\n",
    "        return tools\n",
    "\n",
    "    async def execute_tool(\n",
    "        self, tool_name: str, arguments: Dict[str, Any], retries: int = 2, delay: float = 1.0\n",
    "    ) -> Any:\n",
    "        \"\"\"æ‰§è¡ŒæŒ‡å®šå·¥å…·ï¼Œå¹¶æ”¯æŒé‡è¯•æœºåˆ¶\n",
    "\n",
    "        Args:\n",
    "            tool_name: å·¥å…·åç§°\n",
    "            arguments: å·¥å…·å‚æ•°\n",
    "            retries: é‡è¯•æ¬¡æ•°\n",
    "            delay: é‡è¯•é—´éš”ç§’æ•°\n",
    "\n",
    "        Returns:\n",
    "            å·¥å…·è°ƒç”¨ç»“æœ\n",
    "        \"\"\"\n",
    "        if not self.session:\n",
    "            raise RuntimeError(f\"Server {self.name} not initialized\")\n",
    "        attempt = 0\n",
    "        while attempt < retries:\n",
    "            try:\n",
    "                logging.info(f\"Executing {tool_name} on server {self.name}...\")\n",
    "                result = await self.session.call_tool(tool_name, arguments)\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                logging.warning(\n",
    "                    f\"Error executing tool: {e}. Attempt {attempt} of {retries}.\"\n",
    "                )\n",
    "                if attempt < retries:\n",
    "                    logging.info(f\"Retrying in {delay} seconds...\")\n",
    "                    await asyncio.sleep(delay)\n",
    "                else:\n",
    "                    logging.error(\"Max retries reached. Failing.\")\n",
    "                    raise\n",
    "\n",
    "    async def cleanup(self) -> None:\n",
    "        \"\"\"æ¸…ç†æœåŠ¡å™¨èµ„æº\"\"\"\n",
    "        async with self._cleanup_lock:\n",
    "            try:\n",
    "                await self.exit_stack.aclose()\n",
    "                self.session = None\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during cleanup of server {self.name}: {e}\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# å·¥å…·å°è£…ç±»\n",
    "# =============================\n",
    "class Tool:\n",
    "    \"\"\"å°è£… MCP è¿”å›çš„å·¥å…·ä¿¡æ¯\"\"\"\n",
    "\n",
    "    def __init__(self, name: str, description: str, input_schema: Dict[str, Any]) -> None:\n",
    "        self.name: str = name\n",
    "        self.description: str = description\n",
    "        self.input_schema: Dict[str, Any] = input_schema\n",
    "\n",
    "    def format_for_llm(self) -> str:\n",
    "        \"\"\"ç”Ÿæˆç”¨äº LLM æç¤ºçš„å·¥å…·æè¿°\"\"\"\n",
    "        args_desc = []\n",
    "        if \"properties\" in self.input_schema:\n",
    "            for param_name, param_info in self.input_schema[\"properties\"].items():\n",
    "                arg_desc = f\"- {param_name}: {param_info.get('description', 'No description')}\"\n",
    "                if param_name in self.input_schema.get(\"required\", []):\n",
    "                    arg_desc += \" (required)\"\n",
    "                args_desc.append(arg_desc)\n",
    "        return f\"\"\"\n",
    "Tool: {self.name}\n",
    "Description: {self.description}\n",
    "Arguments:\n",
    "{chr(10).join(args_desc)}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# =============================\n",
    "# LLM å®¢æˆ·ç«¯å°è£…ç±»ï¼ˆä½¿ç”¨ OpenAI SDKï¼‰\n",
    "# =============================\n",
    "class LLMClient:\n",
    "    \"\"\"ä½¿ç”¨ OpenAI SDK ä¸å¤§æ¨¡å‹äº¤äº’\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, base_url: Optional[str], model: str) -> None:\n",
    "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.model = model\n",
    "\n",
    "    def get_response(self, messages: List[Dict[str, Any]], tools: Optional[List[Dict[str, Any]]] = None) -> Any:\n",
    "        \"\"\"\n",
    "        å‘é€æ¶ˆæ¯ç»™å¤§æ¨¡å‹ APIï¼Œæ”¯æŒä¼ å…¥å·¥å…·å‚æ•°ï¼ˆfunction calling æ ¼å¼ï¼‰\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"tools\": tools,\n",
    "        }\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(**payload)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during LLM call: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# =============================\n",
    "# å¤šæœåŠ¡å™¨ MCP å®¢æˆ·ç«¯ç±»ï¼ˆé›†æˆé…ç½®æ–‡ä»¶ã€å·¥å…·æ ¼å¼è½¬æ¢ä¸ OpenAI SDK è°ƒç”¨ï¼‰\n",
    "# =============================\n",
    "class MultiServerMCPClient:\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        ç®¡ç†å¤šä¸ª MCP æœåŠ¡å™¨ï¼Œå¹¶ä½¿ç”¨ OpenAI Function Calling é£æ ¼çš„æ¥å£è°ƒç”¨å¤§æ¨¡å‹\n",
    "        \"\"\"\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        config = Configuration()\n",
    "        self.openai_api_key = config.api_key\n",
    "        self.base_url = config.base_url\n",
    "        self.model = config.model\n",
    "        self.client = LLMClient(self.openai_api_key, self.base_url, self.model)\n",
    "        # (server_name -> Server å¯¹è±¡)\n",
    "        self.servers: Dict[str, Server] = {}\n",
    "        # å„ä¸ª server çš„å·¥å…·åˆ—è¡¨\n",
    "        self.tools_by_server: Dict[str, List[Any]] = {}\n",
    "        self.all_tools: List[Dict[str, Any]] = []\n",
    "\n",
    "    async def connect_to_servers(self, servers_config: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        æ ¹æ®é…ç½®æ–‡ä»¶åŒæ—¶å¯åŠ¨å¤šä¸ªæœåŠ¡å™¨å¹¶è·å–å·¥å…·\n",
    "        servers_config çš„æ ¼å¼ä¸ºï¼š\n",
    "        {\n",
    "          \"mcpServers\": {\n",
    "              \"sqlite\": { \"command\": \"uvx\", \"args\": [ ... ] },\n",
    "              \"puppeteer\": { \"command\": \"npx\", \"args\": [ ... ] },\n",
    "              ...\n",
    "          }\n",
    "        }\n",
    "        \"\"\"\n",
    "        mcp_servers = servers_config.get(\"mcpServers\", {})\n",
    "        for server_name, srv_config in mcp_servers.items():\n",
    "            server = Server(server_name, srv_config)\n",
    "            await server.initialize()\n",
    "            self.servers[server_name] = server\n",
    "            tools = await server.list_tools()\n",
    "            self.tools_by_server[server_name] = tools\n",
    "\n",
    "            for tool in tools:\n",
    "                # ç»Ÿä¸€é‡å‘½åï¼šserverName_toolName\n",
    "                function_name = f\"{server_name}_{tool.name}\"\n",
    "                self.all_tools.append({\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": function_name,\n",
    "                        \"description\": tool.description,\n",
    "                        \"input_schema\": tool.input_schema\n",
    "                    }\n",
    "                })\n",
    "\n",
    "        # è½¬æ¢ä¸º OpenAI Function Calling æ‰€éœ€æ ¼å¼\n",
    "        self.all_tools = await self.transform_json(self.all_tools)\n",
    "\n",
    "        logging.info(\"\\nâœ… å·²è¿æ¥åˆ°ä¸‹åˆ—æœåŠ¡å™¨:\")\n",
    "        for name in self.servers:\n",
    "            srv_cfg = mcp_servers[name]\n",
    "            logging.info(f\"  - {name}: command={srv_cfg['command']}, args={srv_cfg['args']}\")\n",
    "        logging.info(\"\\næ±‡æ€»çš„å·¥å…·:\")\n",
    "        for t in self.all_tools:\n",
    "            logging.info(f\"  - {t['function']['name']}\")\n",
    "\n",
    "    async def transform_json(self, json_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        å°†å·¥å…·çš„ input_schema è½¬æ¢ä¸º OpenAI æ‰€éœ€çš„ parameters æ ¼å¼ï¼Œå¹¶åˆ é™¤å¤šä½™å­—æ®µ\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for item in json_data:\n",
    "            if not isinstance(item, dict) or \"type\" not in item or \"function\" not in item:\n",
    "                continue\n",
    "            old_func = item[\"function\"]\n",
    "            if not isinstance(old_func, dict) or \"name\" not in old_func or \"description\" not in old_func:\n",
    "                continue\n",
    "            new_func = {\n",
    "                \"name\": old_func[\"name\"],\n",
    "                \"description\": old_func[\"description\"],\n",
    "                \"parameters\": {}\n",
    "            }\n",
    "            if \"input_schema\" in old_func and isinstance(old_func[\"input_schema\"], dict):\n",
    "                old_schema = old_func[\"input_schema\"]\n",
    "                new_func[\"parameters\"][\"type\"] = old_schema.get(\"type\", \"object\")\n",
    "                new_func[\"parameters\"][\"properties\"] = old_schema.get(\"properties\", {})\n",
    "                new_func[\"parameters\"][\"required\"] = old_schema.get(\"required\", [])\n",
    "            new_item = {\n",
    "                \"type\": item[\"type\"],\n",
    "                \"function\": new_func\n",
    "            }\n",
    "            result.append(new_item)\n",
    "        return result\n",
    "\n",
    "    async def chat_base(self, messages: List[Dict[str, Any]]) -> Any:\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨ OpenAI æ¥å£è¿›è¡Œå¯¹è¯ï¼Œå¹¶æ”¯æŒå¤šæ¬¡å·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰ã€‚\n",
    "        å¦‚æœè¿”å› finish_reason ä¸º \"tool_calls\"ï¼Œåˆ™è¿›è¡Œå·¥å…·è°ƒç”¨åå†å‘èµ·è¯·æ±‚ã€‚\n",
    "        \"\"\"\n",
    "        response = self.client.get_response(messages, tools=self.all_tools)\n",
    "        # å¦‚æœæ¨¡å‹è¿”å›å·¥å…·è°ƒç”¨\n",
    "        if response.choices[0].finish_reason == \"tool_calls\":\n",
    "            while True:\n",
    "                messages = await self.create_function_response_messages(messages, response)\n",
    "                response = self.client.get_response(messages, tools=self.all_tools)\n",
    "                if response.choices[0].finish_reason != \"tool_calls\":\n",
    "                    break\n",
    "        return response\n",
    "\n",
    "    async def create_function_response_messages(self, messages: List[Dict[str, Any]], response: Any) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        å°†æ¨¡å‹è¿”å›çš„å·¥å…·è°ƒç”¨è§£ææ‰§è¡Œï¼Œå¹¶å°†ç»“æœè¿½åŠ åˆ°æ¶ˆæ¯é˜Ÿåˆ—ä¸­\n",
    "        \"\"\"\n",
    "        function_call_messages = response.choices[0].message.tool_calls\n",
    "        messages.append(response.choices[0].message.model_dump())\n",
    "        for function_call_message in function_call_messages:\n",
    "            tool_name = function_call_message.function.name\n",
    "            tool_args = json.loads(function_call_message.function.arguments)\n",
    "            # è°ƒç”¨ MCP å·¥å…·\n",
    "            function_response = await self._call_mcp_tool(tool_name, tool_args)\n",
    "            # ğŸ” æ‰“å°è¿”å›å€¼åŠå…¶ç±»å‹\n",
    "            # print(f\"[DEBUG] tool_name: {tool_name}\")\n",
    "            # print(f\"[DEBUG] tool_args: {tool_args}\")\n",
    "            # print(f\"[DEBUG] function_response: {function_response}\")\n",
    "            # print(f\"[DEBUG] type(function_response): {type(function_response)}\")\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": function_response,\n",
    "                \"tool_call_id\": function_call_message.id,\n",
    "            })\n",
    "        return messages\n",
    "\n",
    "    async def process_query(self, user_query: str) -> str:\n",
    "        \"\"\"\n",
    "        OpenAI Function Calling æµç¨‹ï¼š\n",
    "         1. å‘é€ç”¨æˆ·æ¶ˆæ¯ + å·¥å…·ä¿¡æ¯\n",
    "         2. è‹¥æ¨¡å‹è¿”å› finish_reason ä¸º \"tool_calls\"ï¼Œåˆ™è§£æå¹¶è°ƒç”¨ MCP å·¥å…·\n",
    "         3. å°†å·¥å…·è°ƒç”¨ç»“æœè¿”å›ç»™æ¨¡å‹ï¼Œè·å¾—æœ€ç»ˆå›ç­”\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_query}]\n",
    "        response = self.client.get_response(messages, tools=self.all_tools)\n",
    "        content = response.choices[0]\n",
    "        logging.info(content)\n",
    "        if content.finish_reason == \"tool_calls\":\n",
    "            tool_call = content.message.tool_calls[0]\n",
    "            tool_name = tool_call.function.name\n",
    "            tool_args = json.loads(tool_call.function.arguments)\n",
    "            logging.info(f\"\\n[ è°ƒç”¨å·¥å…·: {tool_name}, å‚æ•°: {tool_args} ]\\n\")\n",
    "            result = await self._call_mcp_tool(tool_name, tool_args)\n",
    "            messages.append(content.message.model_dump())\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": result,\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "            })\n",
    "            response = self.client.get_response(messages, tools=self.all_tools)\n",
    "            return response.choices[0].message.content\n",
    "        return content.message.content\n",
    "\n",
    "    async def _call_mcp_tool(self, tool_full_name: str, tool_args: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        æ ¹æ® \"serverName_toolName\" æ ¼å¼è°ƒç”¨ç›¸åº” MCP å·¥å…·\n",
    "        \"\"\"\n",
    "        parts = tool_full_name.split(\"_\", 1)\n",
    "        if len(parts) != 2:\n",
    "            return f\"æ— æ•ˆçš„å·¥å…·åç§°: {tool_full_name}\"\n",
    "        server_name, tool_name = parts\n",
    "        server = self.servers.get(server_name)\n",
    "        if not server:\n",
    "            return f\"æ‰¾ä¸åˆ°æœåŠ¡å™¨: {server_name}\"\n",
    "        resp = await server.execute_tool(tool_name, tool_args)\n",
    "        \n",
    "        # ğŸ› ï¸ ä¿®å¤ç‚¹ï¼šæå– TextContent ä¸­çš„æ–‡æœ¬ï¼ˆæˆ–è½¬æˆå­—ç¬¦ä¸²ï¼‰\n",
    "        content = resp.content\n",
    "        if isinstance(content, list):\n",
    "            # æå–æ‰€æœ‰ TextContent å¯¹è±¡ä¸­çš„ text å­—æ®µ\n",
    "            texts = [c.text for c in content if hasattr(c, \"text\")]\n",
    "            return \"\\n\".join(texts)\n",
    "        elif isinstance(content, dict) or isinstance(content, list):\n",
    "            # å¦‚æœæ˜¯ dict æˆ– listï¼Œä½†ä¸æ˜¯ TextContent ç±»å‹\n",
    "            return json.dumps(content, ensure_ascii=False)\n",
    "        elif content is None:\n",
    "            return \"å·¥å…·æ‰§è¡Œæ— è¾“å‡º\"\n",
    "        else:\n",
    "            return str(content)\n",
    "\n",
    "    async def chat_loop(self) -> None:\n",
    "        \"\"\"å¤šæœåŠ¡å™¨ MCP + OpenAI Function Calling å®¢æˆ·ç«¯ä¸»å¾ªç¯\"\"\"\n",
    "        logging.info(\"\\nğŸ¤– å¤šæœåŠ¡å™¨ MCP + Function Calling å®¢æˆ·ç«¯å·²å¯åŠ¨ï¼è¾“å…¥ 'quit' é€€å‡ºã€‚\")\n",
    "        messages: List[Dict[str, Any]] = []\n",
    "        while True:\n",
    "            query = input(\"\\nä½ : \").strip()\n",
    "            if query.lower() == \"quit\":\n",
    "                break\n",
    "            try:\n",
    "                messages.append({\"role\": \"user\", \"content\": query})\n",
    "                messages = messages[-20:]  # ä¿æŒæœ€æ–° 20 æ¡ä¸Šä¸‹æ–‡\n",
    "                response = await self.chat_base(messages)\n",
    "                messages.append(response.choices[0].message.model_dump())\n",
    "                result = response.choices[0].message.content\n",
    "                # logging.info(f\"\\nAI: {result}\")\n",
    "                print(f\"\\nAI: {result}\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\nâš ï¸  è°ƒç”¨è¿‡ç¨‹å‡ºé”™: {e}\")\n",
    "\n",
    "    async def cleanup(self) -> None:\n",
    "        \"\"\"å…³é—­æ‰€æœ‰èµ„æº\"\"\"\n",
    "        await self.exit_stack.aclose()\n",
    "\n",
    "\n",
    "# =============================\n",
    "# ä¸»å‡½æ•°\n",
    "# =============================\n",
    "async def main() -> None:\n",
    "    # ä»é…ç½®æ–‡ä»¶åŠ è½½æœåŠ¡å™¨é…ç½®\n",
    "    config = Configuration()\n",
    "    servers_config = config.load_config(\"servers_config.json\")\n",
    "    client = MultiServerMCPClient()\n",
    "    try:\n",
    "        await client.connect_to_servers(servers_config)\n",
    "        await client.chat_loop()\n",
    "    finally:\n",
    "        try:\n",
    "            await asyncio.sleep(0.1)\n",
    "            await client.cleanup()\n",
    "        except RuntimeError as e:\n",
    "            # å¦‚æœæ˜¯å› ä¸ºé€€å‡º cancel scope å¯¼è‡´çš„å¼‚å¸¸ï¼Œå¯ä»¥é€‰æ‹©å¿½ç•¥\n",
    "            if \"Attempted to exit cancel scope\" in str(e):\n",
    "                logging.info(\"é€€å‡ºæ—¶æ£€æµ‹åˆ° cancel scope å¼‚å¸¸ï¼Œå·²å¿½ç•¥ã€‚\")\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a7c676-e313-415c-b094-bc6299e52cc0",
   "metadata": {},
   "source": [
    "å¤šæœåŠ¡å™¨ MCP + LangChain Agent ç¤ºä¾‹\n",
    "---------------------------------\n",
    "1. è¯»å– .env ä¸­çš„ LLM_API_KEY / BASE_URL / MODEL\n",
    "2. è¯»å– servers_config.json ä¸­çš„ MCP æœåŠ¡å™¨ä¿¡æ¯\n",
    "3. å¯åŠ¨ MCP æœåŠ¡å™¨ï¼ˆæ”¯æŒå¤šä¸ªï¼‰\n",
    "4. å°†æ‰€æœ‰å·¥å…·æ³¨å…¥ LangChain Agentï¼Œç”±å¤§æ¨¡å‹è‡ªåŠ¨é€‰æ‹©å¹¶è°ƒç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6fa2e8-6c3d-4647-b860-75d40873587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ç¯å¢ƒé…ç½®\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class Configuration:\n",
    "    \"\"\"è¯»å– .env ä¸ servers_config.json\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        load_dotenv()\n",
    "        self.api_key: str = os.getenv(\"LLM_API_KEY\") or \"\"\n",
    "        self.base_url: str | None = os.getenv(\"BASE_URL\")  # DeepSeek ç”¨ https://api.deepseek.com\n",
    "        self.model: str = os.getenv(\"MODEL\") or \"deepseek-chat\"\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"âŒ æœªæ‰¾åˆ° LLM_API_KEYï¼Œè¯·åœ¨ .env ä¸­é…ç½®\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_servers(file_path: str = \"servers_config.json\") -> Dict[str, Any]:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f).get(\"mcpServers\", {})\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ä¸»é€»è¾‘\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "async def run_chat_loop() -> None:\n",
    "    \"\"\"å¯åŠ¨ MCP-Agent èŠå¤©å¾ªç¯\"\"\"\n",
    "    cfg = Configuration()\n",
    "    os.environ[\"DEEPSEEK_API_KEY\"] = os.getenv(\"LLM_API_KEY\", \"\")\n",
    "    if cfg.base_url:\n",
    "        os.environ[\"DEEPSEEK_API_BASE\"] = cfg.base_url\n",
    "    servers_cfg = Configuration.load_servers()\n",
    "\n",
    "    # æŠŠ key æ³¨å…¥ç¯å¢ƒï¼ŒLangChain-OpenAI / DeepSeek ä¼šè‡ªåŠ¨è¯»å–\n",
    "    os.environ[\"OPENAI_API_KEY\"] = cfg.api_key\n",
    "    if cfg.base_url:  # å¯¹ DeepSeek ä¹‹ç±»çš„è‡ªå®šä¹‰åŸŸåå¾ˆæœ‰ç”¨\n",
    "        os.environ[\"OPENAI_BASE_URL\"] = cfg.base_url\n",
    "\n",
    "    # 1ï¸âƒ£ è¿æ¥å¤šå° MCP æœåŠ¡å™¨\n",
    "    mcp_client = MultiServerMCPClient(servers_cfg)\n",
    "\n",
    "    tools = await mcp_client.get_tools()         # LangChain Tool å¯¹è±¡åˆ—è¡¨\n",
    "\n",
    "    logging.info(f\"âœ… å·²åŠ è½½ {len(tools)} ä¸ª MCP å·¥å…·ï¼š {[t.name for t in tools]}\")\n",
    "\n",
    "    # 2ï¸âƒ£ åˆå§‹åŒ–å¤§æ¨¡å‹ï¼ˆDeepSeek / OpenAI / ä»»æ„å…¼å®¹ OpenAI åè®®çš„æ¨¡å‹ï¼‰\n",
    "    llm = init_chat_model(\n",
    "        model=cfg.model,\n",
    "        model_provider=\"deepseek\" if \"deepseek\" in cfg.model else \"openai\",\n",
    "    )\n",
    "\n",
    "    # 3ï¸âƒ£ æ„é€  LangChain Agentï¼ˆç”¨é€šç”¨ promptï¼‰\n",
    "    prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "    # 4ï¸âƒ£ CLI èŠå¤©\n",
    "    print(\"\\nğŸ¤– MCP Agent å·²å¯åŠ¨ï¼Œè¾“å…¥ 'quit' é€€å‡º\")\n",
    "    while True:\n",
    "        user_input = input(\"\\nä½ : \").strip()\n",
    "        if user_input.lower() == \"quit\":\n",
    "            break\n",
    "        try:\n",
    "            result = await agent_executor.ainvoke({\"input\": user_input})\n",
    "            print(f\"\\nAI: {result['output']}\")\n",
    "        except Exception as exc:\n",
    "            print(f\"\\nâš ï¸  å‡ºé”™: {exc}\")\n",
    "\n",
    "    # 5ï¸âƒ£ æ¸…ç†\n",
    "    await mcp_client.cleanup()\n",
    "    print(\"ğŸ§¹ èµ„æºå·²æ¸…ç†ï¼ŒBye!\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# å…¥å£\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    asyncio.run(run_chat_loop())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69041642-1caf-41ec-b2e3-24888043757c",
   "metadata": {},
   "source": [
    "LangChainæ¥å…¥MCPçš„æ ¸å¿ƒåŸç†ä¸ºï¼š weather_server.py â†’ å¯åŠ¨ä¸ºå­è¿›ç¨‹ â†’ stdio é€šä¿¡ â†’ MCP åè®® â†’ è½¬æ¢ä¸º LangChain å·¥å…· â†’ LangChain Agent æ‰§è¡Œè¯»å†™ï¼Œæ ¸å¿ƒè½¬æ¢è¿‡ç¨‹ä¸ºï¼šï¼š\n",
    "1. @mcp.tool() â†’ æ ‡å‡† LangChain Tool\n",
    "2. stdio_client() â†’ è‡ªåŠ¨å¤„ç† read/write æµï¼Œå…¶ä¸­read è¡¨ç¤ºä» MCP æœåŠ¡å™¨è¯»å–å“åº”çš„æµï¼Œwrite è¡¨ç¤ºå‘ MCP æœåŠ¡å™¨å‘é€è¯·æ±‚çš„æµï¼Œå¯¹äº stdio weather_server.pyï¼Œå®ƒä»¬å°±æ˜¯å­è¿›ç¨‹çš„ stdout å’Œ stdin\n",
    "3. load_mcp_tools() â†’ ä¸€é”®è½¬æ¢æ‰€æœ‰å·¥å…·"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
