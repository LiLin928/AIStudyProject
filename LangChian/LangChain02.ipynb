{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a374c084-32d0-4677-9d52-99dc0f6b0ab9",
   "metadata": {},
   "source": [
    "### LangChianæ ¸å¿ƒåŠŸèƒ½ï¼šé“¾å¼è°ƒç”¨å®ç°æ–¹æ³•\n",
    "#### - å°è¯•æ­å»ºä¸€ä¸ªç®€å•çš„é“¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9a8bf9-ac38-4940-8c41-3c38ed5ddc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv(override=True)\n",
    "\n",
    "DeepSeek_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd276808-820a-42e2-9b8a-269c22dfe639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# ä½¿ç”¨ DeepSeek æ¨¡å‹\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")  \n",
    "\n",
    "# ç›´æ¥ä½¿ç”¨æ¨¡å‹ + è¾“å‡ºè§£æå™¨æ­å»ºä¸€ä¸ªé“¾\n",
    "basic_qa_chain = model | StrOutputParser()\n",
    "# æŸ¥çœ‹è¾“å‡ºç»“æœ\n",
    "question = \"ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚\"\n",
    "result = basic_qa_chain.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21821fe5-fc97-4e48-baa9-682756131c22",
   "metadata": {},
   "source": [
    "#### - åŠ å…¥æç¤ºè¯æ¨¡æ¿åˆ›å»ºé“¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c48f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.boolean import BooleanOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªä¹æ„åŠ©äººçš„åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºå›ç­”\"),\n",
    "    (\"user\", \"è¿™æ˜¯ç”¨æˆ·çš„é—®é¢˜ï¼š {topic}ï¼Œ è¯·ç”¨ yes æˆ– no æ¥å›ç­”\")\n",
    "])\n",
    "\n",
    "# ç›´æ¥ä½¿ç”¨æ¨¡å‹ + è¾“å‡ºè§£æå™¨\n",
    "bool_qa_chain = prompt_template | model | BooleanOutputParser()\n",
    "# æµ‹è¯•\n",
    "question = \"è¯·é—® 1 + 1 æ˜¯å¦ å¤§äº 2ï¼Ÿ\"\n",
    "result = bool_qa_chain.invoke(question)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b4fe3",
   "metadata": {},
   "source": [
    "#### å€ŸåŠ©æç¤ºè¯æ¨¡æ¿å’Œç»“æœè§£æå™¨å®ç°åŠŸèƒ½æ›´åŠ å¤æ‚çš„é“¾\n",
    "è‡³æ­¤ï¼Œæˆ‘ä»¬å°±æ­å»ºäº†ä¸€ä¸ªéå¸¸åŸºç¡€çš„é“¾ã€‚åœ¨LangChainä¸­ï¼Œä¸€ä¸ªåŸºç¡€çš„é“¾ä¸»è¦ç”±ä¸‰éƒ¨åˆ†æ„æˆï¼Œåˆ†åˆ«æ˜¯æç¤ºè¯æ¨¡æ¿ã€å¤§æ¨¡å‹å’Œç»“æœè§£æå™¨ï¼ˆç»“æ„åŒ–è§£æå™¨ï¼‰ï¼š\n",
    "\n",
    "ç”¨æˆ·è¾“å…¥\n",
    "\n",
    "â†“\n",
    "\n",
    "PromptTemplate â†’ ChatModel â†’ OutputParser\n",
    "ï¼ˆæç¤ºè¯æ¨¡æ¿ï¼‰ ï¼ˆå¤§æ¨¡å‹ï¼‰ ï¼ˆç»“æ„åŒ–è§£æï¼‰\n",
    "\n",
    "â†“\n",
    "\n",
    "æ„åŒ–ç»“æœ\n",
    "\n",
    "è€Œç›¸æ¯”ä¹‹ä¸‹ï¼Œç»“æ„åŒ–è§£æå™¨åŠŸèƒ½æœ€å¤šï¼Œä¸€äº›æ ¸å¿ƒçš„ç»“æ„åŒ–è§£æå™¨åŠŸèƒ½å¦‚ä¸‹ï¼š\n",
    "\n",
    "| è§£æå™¨åç§° | åŠŸèƒ½æè¿° | ç±»å‹ |\n",
    "|------------|----------|------|\n",
    "|BooleanOutputParser|å°†LLMè¾“å‡ºè§£æä¸ºå¸ƒå°”å€¼|åŸºç¡€ç±»å‹è§£æ|\n",
    "|DatetimeOutputParser|å°†LLMè¾“å‡ºè§£æä¸ºæ—¥æœŸæ—¶é—´|åŸºç¡€ç±»å‹è§£æ|\n",
    "|EnumOutputParser|è§£æè¾“å‡ºä¸ºé¢„å®šä¹‰æšä¸¾å€¼ä¹‹ä¸€|åŸºç¡€ç±»å‹è§£æ|\n",
    "|RegexParser|ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æLLMè¾“å‡º|æ¨¡å¼åŒ¹é…è§£æ|\n",
    "|RegexDictParser|ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†è¾“å‡ºè§£æä¸ºå­—å…¸|æ¨¡å¼åŒ¹é…è§£æ|\n",
    "|StructuredOutputParser|å°†LLMè¾“å‡ºè§£æä¸ºç»“æ„åŒ–æ ¼å¼|ç»“æ„åŒ–è§£æ|\n",
    "|YamlOutputParser|ä½¿ç”¨Pydanticæ¨¡å‹è§£æYAMLè¾“å‡º|ç»“æ„åŒ–è§£æ|\n",
    "|PandasDataFrameOutputParser|ä½¿ç”¨Pandas DataFrameæ ¼å¼è§£æè¾“å‡º|æ•°æ®å¤„ç†è§£æ|\n",
    "|CombiningOutputParser|å°†å¤šä¸ªè¾“å‡ºè§£æå™¨ç»„åˆä¸ºä¸€ä¸ª|ç»„åˆè§£æå™¨|\n",
    "|OutputFixingParser|åŒ…è£…è§£æå™¨å¹¶å°è¯•ä¿®å¤è§£æé”™è¯¯|é”™è¯¯å¤„ç†è§£æ|\n",
    "|RetryOutputParser|åŒ…è£…è§£æå™¨å¹¶å°è¯•ä¿®å¤è§£æé”™è¯¯|é”™è¯¯å¤„ç†è§£æ|\n",
    "|RetryWithErrorOutputParser|åŒ…è£…è§£æå™¨å¹¶å°è¯•ä¿®å¤è§£æé”™è¯¯|é”™è¯¯å¤„ç†è§£æ|\n",
    "|ResponseSchema|ç»“æ„åŒ–è¾“å‡ºè§£æå™¨çš„å“åº”æ¨¡å¼|è¾…åŠ©ç±»|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "schemas = [\n",
    "    ResponseSchema(name=\"name\", description=\"ç”¨æˆ·çš„å§“å\"),\n",
    "    ResponseSchema(name=\"age\", description=\"ç”¨æˆ·çš„å¹´é¾„\")\n",
    "]\n",
    "parser = StructuredOutputParser.from_response_schemas(schemas)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"è¯·æ ¹æ®ä»¥ä¸‹å†…å®¹æå–ç”¨æˆ·ä¿¡æ¯ï¼Œå¹¶è¿”å› JSON æ ¼å¼ï¼š\\n{input}\\n\\n{format_instructions}\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"input\": \"ç”¨æˆ·å«æé›·ï¼Œä»Šå¹´25å²ï¼Œæ˜¯ä¸€åå·¥ç¨‹å¸ˆã€‚\"})\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacf985b",
   "metadata": {},
   "source": [
    "####  åˆ›å»ºå¤åˆé“¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef42e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "# ç¬¬ä¸€æ­¥ï¼šæ ¹æ®æ ‡é¢˜ç”Ÿæˆæ–°é—»æ­£æ–‡\n",
    "news_gen_prompt = PromptTemplate.from_template(\n",
    "    \"è¯·æ ¹æ®ä»¥ä¸‹æ–°é—»æ ‡é¢˜æ’°å†™ä¸€æ®µç®€çŸ­çš„æ–°é—»å†…å®¹ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼š\\n\\næ ‡é¢˜ï¼š{title}\"\n",
    ")\n",
    "\n",
    "# ç¬¬ä¸€ä¸ªå­é“¾ï¼šç”Ÿæˆæ–°é—»å†…å®¹\n",
    "news_chain = news_gen_prompt | model\n",
    "\n",
    "# ç¬¬äºŒæ­¥ï¼šä»æ­£æ–‡ä¸­æå–ç»“æ„åŒ–å­—æ®µ\n",
    "schemas = [\n",
    "    ResponseSchema(name=\"time\", description=\"äº‹ä»¶å‘ç”Ÿçš„æ—¶é—´\"),\n",
    "    ResponseSchema(name=\"location\", description=\"äº‹ä»¶å‘ç”Ÿçš„åœ°ç‚¹\"),\n",
    "    ResponseSchema(name=\"event\", description=\"å‘ç”Ÿçš„å…·ä½“äº‹ä»¶\"),\n",
    "]\n",
    "parser = StructuredOutputParser.from_response_schemas(schemas)\n",
    "\n",
    "summary_prompt = PromptTemplate.from_template(\n",
    "    \"è¯·ä»ä¸‹é¢è¿™æ®µæ–°é—»å†…å®¹ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œå¹¶è¿”å›ç»“æ„åŒ–JSONæ ¼å¼ï¼š\\n\\n{news}\\n\\n{format_instructions}\"\n",
    ")\n",
    "\n",
    "# ç¬¬äºŒä¸ªå­é“¾ï¼šç”Ÿæˆæ–°é—»æ‘˜è¦\n",
    "summary_chain = (\n",
    "    summary_prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# ç»„åˆæˆä¸€ä¸ªå¤åˆ Chain\n",
    "full_chain = news_chain | summary_chain\n",
    "\n",
    "# è°ƒç”¨å¤åˆé“¾\n",
    "result = full_chain.invoke({\"title\": \"è‹¹æœå…¬å¸åœ¨åŠ å·å‘å¸ƒæ–°æ¬¾AIèŠ¯ç‰‡\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5413f2b8",
   "metadata": {},
   "source": [
    "#### å€ŸåŠ©LangChainé€‚é…å™¨è®¾ç½®è‡ªå®šä¹‰å¯è¿è¡Œçš„èŠ‚ç‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bb5aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# ä¸€ä¸ªç®€å•çš„æ‰“å°å‡½æ•°ï¼Œè°ƒè¯•ç”¨\n",
    "def debug_print(x):\n",
    "    print(\"ä¸­é—´ç»“æœï¼ˆæ–°é—»æ­£æ–‡ï¼‰:\", x)\n",
    "    return x\n",
    "\n",
    "debug_node = RunnableLambda(debug_print)\n",
    "\n",
    "# æ’å…¥ debug èŠ‚ç‚¹\n",
    "full_chain = news_chain | debug_node | summary_chain\n",
    "# è°ƒç”¨å¤åˆé“¾\n",
    "result = full_chain.invoke({\"title\": \"è‹¹æœå…¬å¸åœ¨åŠ å·å‘å¸ƒæ–°æ¬¾AIèŠ¯ç‰‡\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0f6737",
   "metadata": {},
   "source": [
    "#### æ„å»ºæµå¼æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42befa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# ä½¿ç”¨ DeepSeek æ¨¡å‹\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")  \n",
    "\n",
    "# ç›´æ¥ä½¿ç”¨æç¤ºæ¨¡ç‰ˆ +æ¨¡å‹ + è¾“å‡ºè§£æå™¨\n",
    "qa_chain_with_system = chatbot_prompt | model | StrOutputParser()\n",
    "\n",
    "# å¼‚æ­¥å®ç°æµå¼è¾“å‡º\n",
    "async for chunk in qa_chain_with_system.astream({\"input\": \"ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae222d81",
   "metadata": {},
   "source": [
    "#### æ„å»ºå¤šè½®å¯¹è¯çš„æµå¼æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# ä½¿ç”¨ DeepSeek æ¨¡å‹\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")  \n",
    "\n",
    "# ç›´æ¥ä½¿ç”¨æ¨¡å‹ + è¾“å‡ºè§£æå™¨\n",
    "basic_qa_chain = chatbot_prompt | model | StrOutputParser()\n",
    "\n",
    "# æµ‹è¯•\n",
    "question = \"ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚\"\n",
    "result = basic_qa_chain.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db813a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®°å¿†å¯¹è¯\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "basic_qa_chain = chatbot_prompt | model | StrOutputParser()\n",
    "# å¢åŠ å¯¹è¯\n",
    "messages_list = [\n",
    "    HumanMessage(content=\"ä½ å¥½ï¼Œæˆ‘å«é™ˆæ˜ï¼Œå¥½ä¹…ä¸è§ã€‚\"),\n",
    "    AIMessage(content=\"ä½ å¥½å‘€ï¼æˆ‘æ˜¯å°æ™ºï¼Œä¸€åä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚å¾ˆé«˜å…´è®¤è¯†ä½ ï¼\"),\n",
    "]\n",
    "\n",
    "question = \"ä½ å¥½ï¼Œè¯·é—®æˆ‘å«ä»€ä¹ˆåå­—ã€‚\"\n",
    "# ç»„åˆ\n",
    "messages_list.append(HumanMessage(content=question))\n",
    "# è°ƒç”¨é“¾\n",
    "result = basic_qa_chain.invoke({\"messages\": messages_list})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220abc9",
   "metadata": {},
   "source": [
    "#### å®Œæ•´çš„å¤šè½®å¯¹è¯å‡½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc450241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model  = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "messages_list = []  # åˆå§‹åŒ–å†å²\n",
    "print(\"ğŸ”¹ è¾“å…¥ exit ç»“æŸå¯¹è¯\")\n",
    "while True:\n",
    "    user_query = input(\"ğŸ‘¤ ä½ ï¼š\")\n",
    "    if user_query.lower() in {\"exit\", \"quit\"}:\n",
    "        break\n",
    "\n",
    "    # 1) è¿½åŠ ç”¨æˆ·æ¶ˆæ¯\n",
    "    messages_list.append(HumanMessage(content=user_query))\n",
    "\n",
    "    # 2) è°ƒç”¨æ¨¡å‹ ç­‰å¾…ç»“æœ\n",
    "    assistant_reply = chain.invoke({\"messages\": messages_list})\n",
    "    print(\"ğŸ¤– å°æ™ºï¼š\", assistant_reply)\n",
    "    # 2) è°ƒç”¨æ¨¡å‹ æµå¼å›å¤\n",
    "    async for chunk in chain.astream({\"messages\": messages_list}):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    # 3) è¿½åŠ  AI å›å¤\n",
    "    messages_list.append(AIMessage(content=assistant_reply))\n",
    "\n",
    "    # 4) ä»…ä¿ç•™æœ€è¿‘ 50 æ¡\n",
    "    messages_list = messages_list[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac9e35",
   "metadata": {},
   "source": [
    "#### ç”¨gradioæ¥å¼€å‘ä¸€ä¸ªæ”¯æŒåœ¨ç½‘é¡µä¸Šè¿›è¡Œäº¤äº’çš„é—®ç­”æœºå™¨äºº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b49161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£… Gradio\n",
    "! uv add gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b150b8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\LangChain\\Jupyter\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\ææ—\\AppData\\Local\\Temp\\ipykernel_17452\\795933737.py:53: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "# åˆ›å»ºé—®ç­”é“¾\n",
    "system_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "qa_chain = system_prompt | model | StrOutputParser()\n",
    "\n",
    "# æµå¼å›åº”å‡½æ•°\n",
    "async def chat_response(message, history):\n",
    "    \"\"\"æµå¼ç”ŸæˆAIå›åº”\"\"\"\n",
    "    partial_message = \"\"\n",
    "    \n",
    "    async for chunk in qa_chain.astream({\"input\": message}):\n",
    "        partial_message += chunk\n",
    "        yield partial_message\n",
    "\n",
    "# åˆ›å»º Gradio ç•Œé¢\n",
    "def create_chatbot():\n",
    "    # è‡ªå®šä¹‰CSSæ ·å¼ - å±…ä¸­æ˜¾ç¤º\n",
    "    css = \"\"\"\n",
    "    .main-container {\n",
    "        max-width: 1200px;\n",
    "        margin: 0 auto;\n",
    "        padding: 20px;\n",
    "    }\n",
    "    .header-text {\n",
    "        text-align: center;\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(title=\"DeepSeek Chat\", css=css) as demo:\n",
    "        with gr.Column(elem_classes=[\"main-container\"]):\n",
    "            # å±…ä¸­æ˜¾ç¤ºæ ‡é¢˜\n",
    "            gr.Markdown(\n",
    "                \"# ğŸ¤– LangChain Bç«™å…¬å¼€è¯¾ Byä¹å¤©Hector \", \n",
    "                elem_classes=[\"header-text\"]\n",
    "            )\n",
    "            gr.Markdown(\n",
    "                \"åŸºäº LangChain LCEL æ„å»ºçš„æµå¼å¯¹è¯æœºå™¨äºº\", \n",
    "                elem_classes=[\"header-text\"]\n",
    "            )\n",
    "            \n",
    "            chatbot = gr.Chatbot(\n",
    "                height=500,\n",
    "                show_copy_button=True,\n",
    "                avatar_images=(\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f464.png\",\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f916.png\"\n",
    "                ),\n",
    "                \n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                msg = gr.Textbox(\n",
    "                    placeholder=\"è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...\",\n",
    "                    container=False,\n",
    "                    scale=7\n",
    "                )\n",
    "                submit = gr.Button(\"å‘é€\", scale=1, variant=\"primary\")\n",
    "                clear = gr.Button(\"æ¸…ç©º\", scale=1)\n",
    "        \n",
    "        # å¤„ç†æ¶ˆæ¯å‘é€\n",
    "        async def respond(message, chat_history):\n",
    "            if not message.strip():\n",
    "                yield \"\", chat_history\n",
    "                return\n",
    "            \n",
    "            # 1. æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²å¹¶ç«‹å³æ˜¾ç¤º\n",
    "            chat_history = chat_history + [(message, None)]\n",
    "            yield \"\", chat_history  # ç«‹å³æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯\n",
    "            \n",
    "            # 2. æµå¼ç”ŸæˆAIå›åº”\n",
    "            async for response in chat_response(message, chat_history):\n",
    "                # æ›´æ–°æœ€åä¸€æ¡æ¶ˆæ¯çš„AIå›åº”\n",
    "                chat_history[-1] = (message, response)\n",
    "                yield \"\", chat_history\n",
    "        \n",
    "        # æ¸…ç©ºå¯¹è¯å†å²çš„å‡½æ•°\n",
    "        def clear_history():\n",
    "            return [], \"\"\n",
    "        \n",
    "        # ç»‘å®šäº‹ä»¶\n",
    "        msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "        submit.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "        clear.click(clear_history, outputs=[chatbot, msg])\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# å¯åŠ¨ç•Œé¢\n",
    "demo = create_chatbot()\n",
    "demo.launch(\n",
    "    server_name=\"0.0.0.0\",\n",
    "    server_port=7860,\n",
    "    share=False,\n",
    "    debug=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc90c6",
   "metadata": {},
   "source": [
    "## gradio + è®°å¿† +  æµå¼è¾“å‡º å®Œæ•´å®ç°çš„ä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2457426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. æ¨¡å‹ã€Promptã€Chain\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),  # æ‰‹åŠ¨ä¼ å…¥å†å²\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = chatbot_prompt | model | parser   # LCEL ç»„åˆ\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Gradio ç»„ä»¶\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSS = \"\"\"\n",
    ".main-container {max-width: 1200px; margin: 0 auto; padding: 20px;}\n",
    ".header-text {text-align: center; margin-bottom: 20px;}\n",
    "\"\"\"\n",
    "\n",
    "def create_chatbot() -> gr.Blocks:\n",
    "    with gr.Blocks(title=\"DeepSeek Chat\", css=CSS) as demo:\n",
    "        with gr.Column(elem_classes=[\"main-container\"]):\n",
    "            gr.Markdown(\"# ğŸ¤– LangChain Bç«™å…¬å¼€è¯¾ Byä¹å¤©Hector\", elem_classes=[\"header-text\"])\n",
    "            gr.Markdown(\"åŸºäº LangChain LCEL æ„å»ºçš„æµå¼å¯¹è¯æœºå™¨äºº\", elem_classes=[\"header-text\"])\n",
    "\n",
    "            chatbot = gr.Chatbot(\n",
    "                height=500,\n",
    "                show_copy_button=True,\n",
    "                avatar_images=(\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f464.png\",\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f916.png\",\n",
    "                ),\n",
    "            )\n",
    "            msg = gr.Textbox(placeholder=\"è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...\", container=False, scale=7)\n",
    "            submit = gr.Button(\"å‘é€\", scale=1, variant=\"primary\")\n",
    "            clear = gr.Button(\"æ¸…ç©º\", scale=1)\n",
    "\n",
    "        # ---------------  çŠ¶æ€ï¼šä¿å­˜ messages_list  ---------------\n",
    "        state = gr.State([])          # è¿™é‡Œå­˜æ”¾çœŸæ­£çš„ Message å¯¹è±¡åˆ—è¡¨\n",
    "\n",
    "        # ---------------  ä¸»å“åº”å‡½æ•°ï¼ˆæµå¼ï¼‰ ----------------------\n",
    "        async def respond(user_msg: str, chat_hist: list, messages_list: list):\n",
    "            # 1) è¾“å…¥ä¸ºç©ºç›´æ¥è¿”å›\n",
    "            if not user_msg.strip():\n",
    "                yield \"\", chat_hist, messages_list\n",
    "                return\n",
    "\n",
    "            # 2) è¿½åŠ ç”¨æˆ·æ¶ˆæ¯\n",
    "            messages_list.append(HumanMessage(content=user_msg))\n",
    "            chat_hist = chat_hist + [(user_msg, None)]\n",
    "            yield \"\", chat_hist, messages_list      # å…ˆæ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯\n",
    "\n",
    "            # 3) æµå¼è°ƒç”¨æ¨¡å‹\n",
    "            partial = \"\"\n",
    "            async for chunk in qa_chain.astream({\"messages\": messages_list}):\n",
    "                partial += chunk\n",
    "                # æ›´æ–°æœ€åä¸€æ¡ AI å›å¤\n",
    "                chat_hist[-1] = (user_msg, partial)\n",
    "                yield \"\", chat_hist, messages_list\n",
    "\n",
    "            # 4) å®Œæ•´å›å¤åŠ å…¥å†å²ï¼Œè£å‰ªåˆ°æœ€è¿‘ 50 æ¡\n",
    "            messages_list.append(AIMessage(content=partial))\n",
    "            messages_list = messages_list[-50:]\n",
    "\n",
    "            # 5) æœ€ç»ˆè¿”å›ï¼ˆGradio éœ€è¦æŠŠæ–°çš„ state ä¼ å›ï¼‰\n",
    "            yield \"\", chat_hist, messages_list\n",
    "\n",
    "        # ---------------  æ¸…ç©ºå‡½æ•° -------------------------------\n",
    "        def clear_history():\n",
    "            return [], \"\", []          # æ¸…ç©º Chatbotã€è¾“å…¥æ¡†ã€messages_list\n",
    "\n",
    "        # ---------------  äº‹ä»¶ç»‘å®š ------------------------------\n",
    "        msg.submit(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "        submit.click(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "        clear.click(clear_history, outputs=[chatbot, msg, state])\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. å¯åŠ¨åº”ç”¨\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "demo = create_chatbot()\n",
    "demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=False, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
